[{"id":3,"pagetitle":"CalibrationErrors.jl","title":"CalibrationErrors.jl","ref":"/CalibrationErrors/stable/#CalibrationErrors.jl","content":" CalibrationErrors.jl Estimation of calibration errors. A package for estimating calibration errors from data sets of predictions and targets."},{"id":4,"pagetitle":"CalibrationErrors.jl","title":"Related packages","ref":"/CalibrationErrors/stable/#Related-packages","content":" Related packages CalibrationTests.jl  implements statistical hypothesis tests of calibration. pycalibration  is a Python interface for CalibrationErrors.jl and CalibrationTests.jl. rcalibration  is an R interface for CalibrationErrors.jl and CalibrationTests.jl."},{"id":5,"pagetitle":"CalibrationErrors.jl","title":"Talk at JuliaCon 2021","ref":"/CalibrationErrors/stable/#Talk-at-JuliaCon-2021","content":" Talk at JuliaCon 2021 The slides of the talk are available as  Pluto notebook ."},{"id":6,"pagetitle":"CalibrationErrors.jl","title":"Citing","ref":"/CalibrationErrors/stable/#Citing","content":" Citing If you use CalibrationErrors.jl as part of your research, teaching, or other activities, please consider citing the following publications: Widmann, D., Lindsten, F., & Zachariah, D. (2019).  Calibration tests in multi-class classification: A unifying framework . In  Advances in Neural Information Processing Systems 32 (NeurIPS 2019)  (pp. 12257–12267). Widmann, D., Lindsten, F., & Zachariah, D. (2021).  Calibration tests beyond classification .  International Conference on Learning Representations (ICLR 2021) ."},{"id":7,"pagetitle":"CalibrationErrors.jl","title":"Acknowledgements","ref":"/CalibrationErrors/stable/#Acknowledgements","content":" Acknowledgements This work was financially supported by the Swedish Research Council via the projects  Learning of Large-Scale Probabilistic Dynamical Models  (contract number: 2016-04278),  Counterfactual Prediction Methods for Heterogeneous Populations  (contract number: 2018-05040), and  Handling Uncertainty in Machine Learning Systems  (contract number: 2020-04122), by the Swedish Foundation for Strategic Research via the project  Probabilistic Modeling and Inference for Machine Learning  (contract number: ICA16-0015), by the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation, and by ELLIIT."},{"id":10,"pagetitle":"Expected calibration error (ECE)","title":"Expected calibration error (ECE)","ref":"/CalibrationErrors/stable/ece/#ece","content":" Expected calibration error (ECE)"},{"id":11,"pagetitle":"Expected calibration error (ECE)","title":"Definition","ref":"/CalibrationErrors/stable/ece/#Definition","content":" Definition A common calibration measure is the so-called expected calibration error (ECE). In its most general form, the ECE with respect to distance measure  $d(p, p')$  is defined [WLZ21]  as \\[\\mathrm{ECE}_d := \\mathbb{E} d\\big(P_X, \\mathrm{law}(Y \\,|\\, P_X)\\big).\\] As implied by its name, the ECE is the expected distance between the left and right hand side of the calibration definition with respect to  $d$ . Usually, the ECE is used to analyze classification models. [GPSW17] [VWALRS19]  In this case,  $P_X$  and  $\\mathrm{law}(Y \\,|\\, P_X)$  can be identified with vectors in the probability simplex and  $d$  can be chosen as a the cityblock distance, the total variation distance, or the squared Euclidean distance. For other probabilistic predictive models such as regression models, one has to choose a more general distance measure  $d$  between probability distributions on the target space since the conditional distributions  $\\mathrm{law}(Y \\,|\\, P_X)$  can be arbitrarily complex in general."},{"id":12,"pagetitle":"Expected calibration error (ECE)","title":"Estimators","ref":"/CalibrationErrors/stable/ece/#Estimators","content":" Estimators The main challenge in the estimation of the ECE is the estimation of the conditional distribution  $\\mathrm{law}(Y \\,|\\, P_X)$  from a finite data set of predictions and corresponding targets. Typically, predictions are binned and empirical estimates of the conditional distributions are calculated for each bin. You can construct such estimators with  ECE ."},{"id":13,"pagetitle":"Expected calibration error (ECE)","title":"CalibrationErrors.ECE","ref":"/CalibrationErrors/stable/ece/#CalibrationErrors.ECE","content":" CalibrationErrors.ECE  —  Type ECE(binning[, distance = TotalVariation()]) Estimator of the expected calibration error (ECE) for a classification model with respect to the given  distance  function using the  binning  algorithm. For classification models, the predictions  $P_{X_i}$  and targets  $Y_i$  are identified with vectors in the probability simplex. The estimator of the ECE is defined as \\[\\frac{1}{B} \\sum_{i=1}^B d\\big(\\overline{P}_i, \\overline{Y}_i\\big),\\] where  $B$  is the number of non-empty bins,  $d$  is the distance function, and  $\\overline{P}_i$  and  $\\overline{Y}_i$  are the average vector of the predictions and the average vector of targets in the  $i$ th bin. By default, the total variation distance is used. The  distance  has to be a function of the form distance(pbar::Vector{<:Real}, ybar::Vector{<:Real}). In particular, distance measures of the package  Distances.jl  are supported. source"},{"id":14,"pagetitle":"Expected calibration error (ECE)","title":"Binning algorithms","ref":"/CalibrationErrors/stable/ece/#Binning-algorithms","content":" Binning algorithms Currently, two binning algorithms are supported.  UniformBinning  is a binning schemes with bins of fixed bins of uniform size whereas  MedianVarianceBinning  splits the validation data set of predictions and targets dynamically to reduce the variance of the predictions."},{"id":15,"pagetitle":"Expected calibration error (ECE)","title":"CalibrationErrors.UniformBinning","ref":"/CalibrationErrors/stable/ece/#CalibrationErrors.UniformBinning","content":" CalibrationErrors.UniformBinning  —  Type UniformBinning(nbins::Int) Binning scheme of the probability simplex with  nbins  bins of uniform width for each component. source"},{"id":16,"pagetitle":"Expected calibration error (ECE)","title":"CalibrationErrors.MedianVarianceBinning","ref":"/CalibrationErrors/stable/ece/#CalibrationErrors.MedianVarianceBinning","content":" CalibrationErrors.MedianVarianceBinning  —  Type MedianVarianceBinning([minsize::Int = 10, maxbins::Int = typemax(Int)]) Dynamic binning scheme of the probability simplex with at most  maxbins  bins that each contain at least  minsize  samples. The data set is split recursively as long as it is possible to split the bins while satisfying these conditions. In each step, the bin with the maximum variance of predicted probabilities for any component is selected and split at the median of the predicted probability of the component with the largest variance. source GPSW17 Guo, C., et al. (2017).  On calibration of modern neural networks . In  Proceedings of the 34th International Conference on Machine Learning  (pp. 1321-1330). VWALRS19 Vaicenavicius, J., et al. (2019).  Evaluating model calibration in classification . In  Proceedings of Machine Learning Research (AISTATS 2019)  (pp. 3459-3467). WLZ21 Widmann, D., Lindsten, F., & Zachariah, D. (2021).  Calibration tests beyond classification . To be presented at  ICLR 2021 ."},{"id":19,"pagetitle":"Classification of penguin species","title":"Classification of penguin species","ref":"/CalibrationErrors/stable/examples/classification/#Classification-of-penguin-species","content":" Classification of penguin species You are seeing the HTML output generated by  Documenter.jl  and  Literate.jl  from the  Julia source file . The corresponding notebook can be viewed in  nbviewer ."},{"id":20,"pagetitle":"Classification of penguin species","title":"Packages","ref":"/CalibrationErrors/stable/examples/classification/#Packages","content":" Packages using AlgebraOfGraphics\nusing CairoMakie\nusing CalibrationErrors\nusing DataFrames\nusing Distributions\nusing MLJ\nusing MLJNaiveBayesInterface\nusing PalmerPenguins\n\nusing Random\n\n# Plotting settings\nset_aog_theme!()\nCairoMakie.activate!(; type=\"svg\")"},{"id":21,"pagetitle":"Classification of penguin species","title":"Data","ref":"/CalibrationErrors/stable/examples/classification/#Data","content":" Data In this example we study the calibration of different models that classify three penguin species based on measurements of their bill and flipper lengths. We use the  Palmer penguins dataset  to to train and validate the models. penguins = dropmissing(DataFrame(PalmerPenguins.load()))\n\npenguins_mapping =\n    data(penguins) * mapping(\n        :bill_length_mm => \"bill length (mm)\", :flipper_length_mm => \"flipper length (mm)\"\n    )\ndraw(penguins_mapping * mapping(; color=:species) * visual(; alpha=0.7)) We split the data randomly into a training and validation dataset. The training dataset contains around 60% of the samples. Random.seed!(1234)\nn = nrow(penguins)\nk = floor(Int, 0.7 * n)\nRandom.seed!(100)\npenguins.train = shuffle!(vcat(trues(k), falses(n - k)))\n\n# Plot the training and validation data\ndataset = :train => renamer(true => \"training\", false => \"validation\") => \"Dataset\"\nplt = penguins_mapping * mapping(; color=:species, col=dataset) * visual(; alpha=0.7)\ndraw(plt; axis=(height=300,))"},{"id":22,"pagetitle":"Classification of penguin species","title":"Fitting normal distributions","ref":"/CalibrationErrors/stable/examples/classification/#Fitting-normal-distributions","content":" Fitting normal distributions For each species, we fit independent normal distributions to the observations of the bill and flipper length in the training data, using maximum likelihood estimation. y, X = unpack(\n    penguins,\n    ==(:species),\n    x -> x === :bill_length_mm || x === :flipper_length_mm;\n    :species => Multiclass,\n    :bill_length_mm => MLJ.Continuous,\n    :flipper_length_mm => MLJ.Continuous,\n)\nmodel = fit!(machine(GaussianNBClassifier(), X, y); rows=penguins.train); [ Info: Training machine(GaussianNBClassifier(), …).\n We plot the estimated normal distributions. # plot datasets\nfg = draw(plt; axis=(height=300,))\n\n# plot Gaussian distributions\nxgrid = range(extrema(penguins.bill_length_mm)...; length=100)\nygrid = range(extrema(penguins.flipper_length_mm)...; length=100)\nlet f = (x, y, dist) -> pdf(dist, [x, y])\n    for (class, color) in zip(classes(y), Makie.wong_colors())\n        pdfs = f.(xgrid, ygrid', Ref(model.fitresult.gaussians[class]))\n        contour!(fg.figure[1, 1], xgrid, ygrid, pdfs; color=color)\n        contour!(fg.figure[1, 2], xgrid, ygrid, pdfs; color=color)\n    end\nend\n\nfg"},{"id":23,"pagetitle":"Classification of penguin species","title":"Naive Bayes classifier","ref":"/CalibrationErrors/stable/examples/classification/#Naive-Bayes-classifier","content":" Naive Bayes classifier Let us assume that the bill and flipper length are conditionally independent given the penguin species. Then Bayes' theorem implies that \\[\\begin{aligned}\n\\mathbb{P}(\\mathrm{species} \\,|\\, \\mathrm{bill}, \\mathrm{flipper})\n&= \\frac{\\mathbb{P}(\\mathrm{species}) \\mathbb{P}(\\mathrm{bill}, \\mathrm{flipper} \\,|\\, \\mathrm{species})}{\\mathbb{P}(\\mathrm{bill}, \\mathrm{flipper})} \\\\\n&= \\frac{\\mathbb{P}(\\mathrm{species}) \\mathbb{P}(\\mathrm{bill} \\,|\\, \\mathrm{species}) \\mathbb{P}(\\mathrm{flipper} \\,|\\, \\mathrm{species})}{\\mathbb{P}(\\mathrm{bill}, \\mathrm{flipper})}.\n\\end{aligned}\\] This predictive model is known as  naive Bayes classifier . In the section above, we estimated  $\\mathbb{P}(\\mathrm{species})$ ,  $\\mathbb{P}(\\mathrm{bill} \\,|\\, \\mathrm{species})$ , and  $\\mathbb{P}(\\mathrm{flipper} \\,|\\, \\mathrm{species})$  for each penguin species from the training data. For the conditional distributions we used a Gaussian approximation. predictions = MLJ.predict(model)\ntrain_predict = predictions[penguins.train]\nval_predict = predictions[.!penguins.train]\n\n# Plot datasets\nfg = draw(plt; axis=(height=300,))\n\n# Plot predictions\npredictions_grid = reshape(\n    MLJ.predict(model, reduce(hcat, vcat.(xgrid, ygrid'))'), length(xgrid), length(ygrid)\n)\nfor (class, color) in zip(classes(y), Makie.wong_colors())\n    p = pdf.(predictions_grid, class)\n    contour!(fg.figure[1, 1], xgrid, ygrid, p; color=color)\n    contour!(fg.figure[1, 2], xgrid, ygrid, p; color=color)\nend\n\nfg"},{"id":24,"pagetitle":"Classification of penguin species","title":"Evaluation","ref":"/CalibrationErrors/stable/examples/classification/#Evaluation","content":" Evaluation We evaluate the probabilistic predictions of the naive Bayes classifier that we just trained."},{"id":25,"pagetitle":"Classification of penguin species","title":"Log-likelihood","ref":"/CalibrationErrors/stable/examples/classification/#Log-likelihood","content":" Log-likelihood We compute the average log-likelihood of the validation data. It is equivalent to the negative cross-entropy. val_y = y[.!penguins.train]\n-mean(cross_entropy(val_predict, val_y)) -0.12188703745583586"},{"id":26,"pagetitle":"Classification of penguin species","title":"Brier score","ref":"/CalibrationErrors/stable/examples/classification/#Brier-score","content":" Brier score The average log-likelihood is also equivalent to the  logarithmic score . The Brier score is another strictly proper scoring rule that can be used for evaluating probabilistic predictions. mean(brier_score(val_predict, val_y)) -0.07385286949971775"},{"id":27,"pagetitle":"Classification of penguin species","title":"Expected calibration error","ref":"/CalibrationErrors/stable/examples/classification/#Expected-calibration-error","content":" Expected calibration error As all proper scoring rules, the logarithmic and the Brier score can be  decomposed in three terms that quantify the sharpness and calibration of the predictive model and the irreducible uncertainty of the targets that is inherent to the prediction problem . The calibration term in this decomposition is the expected calibration error (ECE) \\[\\mathbb{E} d\\big(P_X, \\mathrm{law}(Y \\,|\\, P_X)\\big)\\] with respect to the score divergence  $d$ . Scoring rules, however, include also the sharpness and the uncertainty term. Thus models can trade off calibration for sharpness and therefore scoring rules are not suitable for specifically evaluating calibration of predictive models. The score divergence to the logarithmic and the Brier score are the Kullback-Leibler (KL) divergence \\[d\\big(P_X, \\mathrm{law}(Y \\,|\\, P_X)\\big) = \\sum_{y} \\mathbb{P}(Y = y \\,|\\, P_X)\n\\log\\big(\\mathbb{P}(Y = y \\,|\\, P_X) / P_X(\\{y\\})\\big)\\] and the squared Euclidean distance \\[d\\big(P_X, \\mathrm{law}(Y \\,|\\, P_X)\\big) = \\sum_{y} \\big(P_X - \\mathrm{law}(Y \\,|\\, P_X)\\big)^2(\\{y\\}),\\] respectively. The KL divergence is defined only if  $\\mathrm{law}(Y \\,|\\, P_X)$  is absolutely continuous with respect to  $P_X$ , i.e., if  $P_X(\\{y\\}) = 0$  implies  $\\mathbb{P}(Y = y \\,|\\, P_X) = 0$ . We estimate the ECE by binning the probability simplex of predictions  $P_X$  and computing the weighted average of the distances between the mean prediction and the distribution of targets in each bin. One approach is to use bins of uniform size. ece = ECE(UniformBinning(10), (μ, y) -> kl_divergence(y, μ)); We have to work with a numerical encoding of the true penguin species and a corresponding vector of predictions. We use  RowVecs  to indicate that the rows in the matrix of probabilities returned by  pdf  are the predictions. If we would provide predictions as columns of a matrix, we would have to use  ColVecs . val_yint = map(MLJ.levelcode, val_y)\nval_probs = RowVecs(pdf(val_predict, MLJ.classes(y))); We compute the estimate on the validation data: ece(val_probs, val_yint) 0.04860861700674836 For the squared Euclidean distance we obtain: ece = ECE(UniformBinning(10), SqEuclidean())\nece(val_probs, val_yint) 0.02426469201343113 Alternatively, one can use a data-dependent binning scheme that tries to split the predictions in a way that minimizes the variance in each bin. With the KL divergence we get: ece = ECE(MedianVarianceBinning(5), (μ, y) -> kl_divergence(y, μ))\nece(val_probs, val_yint) 0.027874966150111966 For the squared Euclidean distance we obtain: ece = ECE(MedianVarianceBinning(5), SqEuclidean())\nece(val_probs, val_yint) 0.012238423729555838 We see that the estimates (of the same theoretical quantity!) are highly dependent on the chosen binning scheme."},{"id":28,"pagetitle":"Classification of penguin species","title":"Kernel calibration error","ref":"/CalibrationErrors/stable/examples/classification/#Kernel-calibration-error","content":" Kernel calibration error As an alternative to the ECE, we estimate the kernel calibration error (KCE). We keep it simple here, and use the tensor product kernel \\[k\\big((\\mu, y), (\\mu', y')\\big) = \\delta_{y,y'} \\exp{\\bigg(-\\frac{{\\|\\mu - \\mu'\\|}_2^2}{2\\nu^2} \\bigg)}\\] with length scale  $\\nu > 0$  for predictions  $\\mu,\\mu'$  and corresponding targets  $y, y'$ . For simplicity, we estimate length scale  $\\nu$  with the median heuristic. distances = pairwise(SqEuclidean(), RowVecs(pdf(train_predict, MLJ.classes(y))))\nν = sqrt(median(distances[i] for i in CartesianIndices(distances) if i[1] < i[2]))\nkernel = with_lengthscale(GaussianKernel(), ν) ⊗ WhiteKernel(); We obtain the following biased estimate of the squared KCE (SKCE): skce = SKCE(kernel; unbiased=false)\nskce(val_probs, val_yint) 0.0007888007181424227 Similar to the biased estimates of the ECE, the biased estimates of the SKCE are always non-negative. The unbiased estimates can be negative as well, in particular if the model is (close to being) calibrated: skce = SKCE(kernel)\nskce(val_probs, val_yint) 5.0779821358820946e-5 When the datasets are large, the quadratic sample complexity of the standard biased and unbiased estimators of the SKCE can become prohibitive. In these cases, one can resort to an estimator that averages estimates of non-overlapping blocks of samples. This estimator allows to trade off computational cost for increased variance. Here we consider the extreme case of blocks with two samples, which yields an estimator with linear sample complexity: skce = SKCE(kernel; blocksize=2)\nskce(val_probs, val_yint) 0.01903306352441053"},{"id":29,"pagetitle":"Classification of penguin species","title":"Package and system information","ref":"/CalibrationErrors/stable/examples/classification/#Package-and-system-information","content":" Package and system information"},{"id":30,"pagetitle":"Classification of penguin species","title":"Package version","ref":"/CalibrationErrors/stable/examples/classification/#Package-version","content":" Package version Status `~/work/CalibrationErrors.jl/CalibrationErrors.jl/examples/classification/Project.toml`\n  [cbdf2221] AlgebraOfGraphics v0.6.16\n  [13f3f980] CairoMakie v0.10.6\n  [33913031] CalibrationErrors v0.6.4\n  [a93c6f00] DataFrames v1.5.0\n  [31c24e10] Distributions v0.25.98\n  [add582a8] MLJ v0.19.2\n  [33e4bacb] MLJNaiveBayesInterface v0.1.6\n  [8b842266] PalmerPenguins v0.1.4"},{"id":31,"pagetitle":"Classification of penguin species","title":"Computer information","ref":"/CalibrationErrors/stable/examples/classification/#Computer-information","content":" Computer information Julia Version 1.9.1\nCommit 147bdf428cd (2023-06-07 08:27 UTC)\nPlatform Info:\n  OS: Linux (x86_64-linux-gnu)\n  CPU: 2 × Intel(R) Xeon(R) CPU E5-2673 v4 @ 2.30GHz\n  WORD_SIZE: 64\n  LIBM: libopenlibm\n  LLVM: libLLVM-14.0.6 (ORCJIT, broadwell)\n  Threads: 1 on 2 virtual cores\nEnvironment:\n  JULIA_PKG_SERVER_REGISTRY_PREFERENCE = eager\n  JULIA_DEBUG = Documenter\n  JULIA_LOAD_PATH = :/home/runner/work/CalibrationErrors.jl/CalibrationErrors.jl/docs"},{"id":32,"pagetitle":"Classification of penguin species","title":"Manifest","ref":"/CalibrationErrors/stable/examples/classification/#Manifest","content":" Manifest To reproduce the project environment of this example you can  download the full Manifest.toml . This page was generated using  Literate.jl ."},{"id":35,"pagetitle":"Distribution of calibration error estimates","title":"Distribution of calibration error estimates","ref":"/CalibrationErrors/stable/examples/distribution/#Distribution-of-calibration-error-estimates","content":" Distribution of calibration error estimates You are seeing the HTML output generated by  Documenter.jl  and  Literate.jl  from the  Julia source file . The corresponding notebook can be viewed in  nbviewer ."},{"id":36,"pagetitle":"Distribution of calibration error estimates","title":"Packages","ref":"/CalibrationErrors/stable/examples/distribution/#Packages","content":" Packages using CairoMakie\nusing CalibrationErrors\nusing Distributions\nusing StatsBase\n\nusing LinearAlgebra\nusing Random\nusing Statistics\n\nCairoMakie.activate!(; type=\"svg\")"},{"id":37,"pagetitle":"Distribution of calibration error estimates","title":"Introduction","ref":"/CalibrationErrors/stable/examples/distribution/#Introduction","content":" Introduction This example is taken from the publication  \"Calibration tests in multi-class classification: A unifying framework\"  by Widmann, Lindsten, and Zachariah (2019). We estimate calibration errors of the model \\[\\begin{aligned}\n   g(X) &\\sim \\mathrm{Dir}(\\alpha),\\\\\n   Z &\\sim \\mathrm{Ber}(\\pi),\\\\\n   Y \\,|\\, g(X) = \\gamma, Z = 1 &\\sim \\mathrm{Categorical}(\\beta),\\\\\n   Y \\,|\\, g(X) = \\gamma, Z = 0 &\\sim \\mathrm{Categorical}(\\gamma),\n\\end{aligned}\\] where  $\\alpha \\in \\mathbb{R}_{>0}^m$  determines the distribution of predictions  $g(X)$ ,  $\\pi > 0$  determines the degree of miscalibration, and  $\\beta$  defines a fixed categorical distribution. Here we consider only the choices  $\\alpha = (0.1, \\ldots, 0.1)$ , mimicking a distribution after training that is pushed towards the edges of the probability simplex, and  $\\beta = (1, 0, \\ldots, 0)$ . In our experiments we sample 250 predictions from the Dirichlet distribution  $\\textrm{Dir}(\\alpha)$ , and then we generate corresponding labels according to the model stated above, for different choices of  $\\pi$  and number of classes  $m$ . We evaluate the standard estimators of expected calibration error (ECE) based on a uniform binning scheme and a data-dependent binning scheme, and the biased estimator of the squared kernel calibration error (SKCE), the quadratic unbiased estimator of the SKCE, and the linear unbiased estimator of the SKCE for a specific choice of matrix-valued kernels. The sampling procedure and the evaluation are repeated 100 times, to obtain a sample of 100 estimates for each considered setting of  $\\pi$  and  $m$ . For our choice of  $\\alpha$  and  $\\beta$ , the analytical ECE with respect to the total variation distance  $\\|.\\|_{\\mathrm{TV}}$  is \\[\\mathrm{ECE}_{\\mathrm{TV}} = \\frac{\\pi(m-1)}{m}.\\]"},{"id":38,"pagetitle":"Distribution of calibration error estimates","title":"Estimates","ref":"/CalibrationErrors/stable/examples/distribution/#Estimates","content":" Estimates function estimates(estimator, π::Real, m::Int)\n    # cache array for predictions, modified predictions, and labels\n    predictions = [Vector{Float64}(undef, m) for _ in 1:250]\n    targets = Vector{Int}(undef, 250)\n    data = (predictions, targets)\n\n    # define sampler of predictions\n    sampler_predictions = sampler(Dirichlet(m, 0.1))\n\n    # initialize estimates\n    estimates = Vector{Float64}(undef, 100)\n\n    # for each run\n    @inbounds for i in eachindex(estimates)\n        # sample predictions\n        rand!.((sampler_predictions,), predictions)\n\n        # sample targets\n        for (j, p) in enumerate(predictions)\n            if rand() < π\n                targets[j] = 1\n            else\n                targets[j] = rand(Categorical(p))\n            end\n        end\n\n        # evaluate estimator\n        estimates[i] = estimator(data)(predictions, targets)\n    end\n\n    return estimates\nend; We use a helper function to run the experiment for all desired parameter settings. struct EstimatesSet\n    m::Vector{Int}\n    π::Vector{Float64}\n    estimates::Matrix{Vector{Float64}}\nend\n\nfunction estimates(estimator)\n    # for all combinations of m and π\n    mvec = [2, 10, 100]\n    πvec = [0.0, 0.5, 1.0]\n    estimatesmat = estimates.((estimator,), πvec', mvec)\n\n    return EstimatesSet(mvec, πvec, estimatesmat)\nend; As mentioned above, we can calculate the analytic expected calibration error. For the squared kernel calibration error, we take the mean of the estimates of the unbiased quadratic estimator as approximation of the true value. We provide simple histogram plots of our results. The mean value of the estimates is indicated by a solid vertical line and the analytic calibration error for the ECE is visualized as a dashed line. function plot_estimates(set::EstimatesSet; ece=false)\n    # create figure\n    f = Figure(; resolution=(1080, 960))\n\n    # add subplots\n    nrows, ncols = size(set.estimates)\n    for (j, π) in enumerate(set.π), (i, m) in enumerate(set.m)\n        # obtain data\n        estimates = set.estimates[i, j]\n\n        # create new axis\n        ax = Axis(f[i, j]; xticks=LinearTicks(4))\n        i < nrows && hidexdecorations!(ax; grid=false)\n        j > 1 && hideydecorations!(ax; grid=false)\n\n        # plot histogram of estimates\n        h = fit(Histogram, estimates)\n        barplot!(ax, h; strokecolor=:black, strokewidth=0.5)\n\n        # indicate mean of estimates\n        vlines!(ax, [mean(estimates)]; linewidth=2)\n\n        # indicate analytic calibration error for ECE\n        if ece\n            vlines!(ax, [π * (m - 1) / m]; linewidth=2, linestyle=:dash)\n        end\n    end\n\n    # add labels and link axes\n    for (j, π) in enumerate(set.π)\n        Box(f[1, j, Top()]; color=:gray90)\n        Label(f[1, j, Top()], \"π = $π\"; padding=(0, 0, 5, 5))\n        linkxaxes!(contents(f[:, j])...)\n    end\n    for (i, m) in enumerate(set.m)\n        Box(f[i, ncols, Right()]; color=:gray90)\n        Label(f[i, ncols, Right()], \"$m classes\"; rotation=-π / 2, padding=(5, 5, 0, 0))\n        linkyaxes!(contents(f[i, :])...)\n    end\n    Label(f[nrows, 1:ncols, Bottom()], \"calibration error estimate\"; padding=(0, 0, 0, 75))\n    Label(f[1:nrows, 1, Left()], \"# runs\"; rotation=π / 2, padding=(0, 75, 0, 0))\n\n    return f\nend;"},{"id":39,"pagetitle":"Distribution of calibration error estimates","title":"Kernel choice","ref":"/CalibrationErrors/stable/examples/distribution/#Kernel-choice","content":" Kernel choice We use a tensor product kernel consisting of an exponential kernel  $k(\\mu, \\mu') = \\exp{(- \\gamma \\|p - p'\\|)}$  on the space of predicted categorical distributions and a white kernel  $k(y, y') = \\delta(y - y')$  on the space of targets  $\\{1,\\ldots,m\\}$ . The total variation distance is chosen as the norm on the space of predictions, and the inverse lengthscale  $\\gamma$  is set according to the median heuristic. struct MedianHeuristicKernel\n    distances::Matrix{Float64}\n    cache::Vector{Float64}\nend\n\nfunction MedianHeuristicKernel(n::Int)\n    return MedianHeuristicKernel(\n        Matrix{Float64}(undef, n, n), Vector{Float64}(undef, (n * (n - 1)) ÷ 2)\n    )\nend\n\nfunction (f::MedianHeuristicKernel)((predictions, targets))\n    distances = f.distances\n    cache = f.cache\n\n    # compute lengthscale with median heuristic\n    pairwise!(distances, TotalVariation(), predictions)\n    k = 0\n    @inbounds for j in axes(distances, 2), i in 1:(j - 1)\n        cache[k += 1] = distances[i, j]\n    end\n    λ = median!(cache)\n\n    # create tensor product kernel\n    kernel_predictions = with_lengthscale(ExponentialKernel(; metric=TotalVariation()), λ)\n    kernel_targets = WhiteKernel()\n\n    return kernel_predictions ⊗ kernel_targets\nend"},{"id":40,"pagetitle":"Distribution of calibration error estimates","title":"Expected calibration error","ref":"/CalibrationErrors/stable/examples/distribution/#Expected-calibration-error","content":" Expected calibration error"},{"id":41,"pagetitle":"Distribution of calibration error estimates","title":"Uniform binning","ref":"/CalibrationErrors/stable/examples/distribution/#Uniform-binning","content":" Uniform binning We start by analyzing the expected calibration error (ECE). For our estimation we use 10 bins of uniform width in each dimension. Random.seed!(1234)\ndata = estimates(_ -> ECE(UniformBinning(10), TotalVariation()))\nplot_estimates(data; ece=true)"},{"id":42,"pagetitle":"Distribution of calibration error estimates","title":"Non-uniform binning","ref":"/CalibrationErrors/stable/examples/distribution/#Non-uniform-binning","content":" Non-uniform binning We repeat our experiments with a different data-dependent binning scheme. This time the bins will be computed dynamically by splitting the predictions at the median of the classes with the highest variance, as long as the number of bins does not exceed a given threshold and the number of samples per bin is above a certain lower bound. In our experiments we do not impose any restriction on the number of bins but only stop splitting if the number of samples is less than 10. Random.seed!(1234)\ndata = estimates(_ -> ECE(MedianVarianceBinning(10), TotalVariation()))\nplot_estimates(data; ece=true)"},{"id":43,"pagetitle":"Distribution of calibration error estimates","title":"Unbiased estimators of the squared kernel calibration error","ref":"/CalibrationErrors/stable/examples/distribution/#Unbiased-estimators-of-the-squared-kernel-calibration-error","content":" Unbiased estimators of the squared kernel calibration error Random.seed!(1234)\ndata = estimates(SKCE ∘ MedianHeuristicKernel(250))\nplot_estimates(data)\n\nRandom.seed!(1234)\ndata = estimates() do predictions_targets\n    return SKCE(MedianHeuristicKernel(250)(predictions_targets); blocksize=2)\nend\nplot_estimates(data)"},{"id":44,"pagetitle":"Distribution of calibration error estimates","title":"Biased estimator of the squared kernel calibration error","ref":"/CalibrationErrors/stable/examples/distribution/#Biased-estimator-of-the-squared-kernel-calibration-error","content":" Biased estimator of the squared kernel calibration error Random.seed!(1234)\ndata = estimates() do predictions_targets\n    return SKCE(MedianHeuristicKernel(250)(predictions_targets); unbiased=false)\nend\nplot_estimates(data)"},{"id":45,"pagetitle":"Distribution of calibration error estimates","title":"Package and system information","ref":"/CalibrationErrors/stable/examples/distribution/#Package-and-system-information","content":" Package and system information"},{"id":46,"pagetitle":"Distribution of calibration error estimates","title":"Package version","ref":"/CalibrationErrors/stable/examples/distribution/#Package-version","content":" Package version Status `~/work/CalibrationErrors.jl/CalibrationErrors.jl/examples/distribution/Project.toml`\n  [13f3f980] CairoMakie v0.10.6\n  [33913031] CalibrationErrors v0.6.4\n  [31c24e10] Distributions v0.25.98\n⌃ [2913bbd2] StatsBase v0.33.21\nInfo Packages marked with ⌃ have new versions available and may be upgradable."},{"id":47,"pagetitle":"Distribution of calibration error estimates","title":"Computer information","ref":"/CalibrationErrors/stable/examples/distribution/#Computer-information","content":" Computer information Julia Version 1.9.1\nCommit 147bdf428cd (2023-06-07 08:27 UTC)\nPlatform Info:\n  OS: Linux (x86_64-linux-gnu)\n  CPU: 2 × Intel(R) Xeon(R) CPU E5-2673 v4 @ 2.30GHz\n  WORD_SIZE: 64\n  LIBM: libopenlibm\n  LLVM: libLLVM-14.0.6 (ORCJIT, broadwell)\n  Threads: 1 on 2 virtual cores\nEnvironment:\n  JULIA_PKG_SERVER_REGISTRY_PREFERENCE = eager\n  JULIA_DEBUG = Documenter\n  JULIA_LOAD_PATH = :/home/runner/work/CalibrationErrors.jl/CalibrationErrors.jl/docs"},{"id":48,"pagetitle":"Distribution of calibration error estimates","title":"Manifest","ref":"/CalibrationErrors/stable/examples/distribution/#Manifest","content":" Manifest To reproduce the project environment of this example you can  download the full Manifest.toml . This page was generated using  Literate.jl ."},{"id":51,"pagetitle":"Introduction","title":"Introduction","ref":"/CalibrationErrors/stable/introduction/#Introduction","content":" Introduction"},{"id":52,"pagetitle":"Introduction","title":"Probabilistic predictive models","ref":"/CalibrationErrors/stable/introduction/#Probabilistic-predictive-models","content":" Probabilistic predictive models A probabilistic predictive model predicts a probability distribution over a set of targets for a given feature. By predicting a distribution, one can express the uncertainty in the prediction, which might be inherent to the prediction task (e.g., if the feature does not contain enough information to determine the target with absolute certainty) or caused by insufficient knowledge of the underlying relation between feature and target (e.g., if only a small number of observations of features and corresponding targets are available). [1] In the  classification example  we study the  Palmer penguins dataset  with measurements of three different penguin species and consider the task of predicting the probability of a penguin species ( target ) given the bill and flipper length ( feature ). For this classification task there exist many different probabilistic predictive models. We denote the feature by  $X$  and the target by  $Y$ , and let  $P_X$  be the prediction of a specific model  $P$  for a feature  $X$ . Ideally, we would like that \\[P_X = \\mathrm{law}(Y \\,|\\, X) \\qquad \\text{almost surely},\\] i.e., the model should predict the law of target  $Y$  given features  $X$ . [2]  Of course, usually it is not possible to achieve this in practice. A very simple class of models are models that yield the same prediction for all features, i.e., they return the same probabilities for the penguin species regardless of the bill and flipper length. Clearly, more complicated models take into account also the features and might output different predictions for different features. In contrast to probabilistic predictive models, non-probabilistic predictive models predict a single target instead of a distribution over targets. In fact, such models can be viewed as a special class of probabilistic predictive models that output only Dirac distributions, i.e., that always predict 100% probability for one penguin species and 0% probability for all others. Some other prediction models output a single target together with a confidence score between 0 and 1. Even these models can be reformulated as probabilistic predictive models, arguably in a slightly unconventional way: they correspond to a probabilistic model for a a binary classification problem whose feature space is extended with the predicted target and whose target is the predicted confidence score."},{"id":53,"pagetitle":"Introduction","title":"Calibration","ref":"/CalibrationErrors/stable/introduction/#Calibration","content":" Calibration The main motivation for using a probabilistic model is that it provides additional information about the uncertainty of the predictions, which is valuable for decision making. A  classic example are weather forecasts  that also report the \"probability of rain\" instead of only if it will rain or not. Therefore it is not sufficient if the model predicts an arbitrary distribution. Instead the predictions should actually express the involved uncertainties \"correctly\". One desired property is that the predictions are consistent: if the forecasts predict an 80% probability of rain for an infinite sequence of days, then ideally on 80% of the days it rains. More generally, mathematically we would like \\[P_X = \\mathrm{law}(Y \\,|\\, P_X) \\quad \\text{almost surely},\\] i.e., the predicted distribution of targets should be equal to the distribution of targets conditioned on the predicted distribution. [3] This statistical property is called  calibration . If it is satisfied, a model is  calibrated . Obviously, the ideal model  $P_X = \\mathrm{law}(Y \\,|\\, X)$  is calibrated. However, also the naive model  $P_X = \\mathrm{law}(Y)$  that always predicts the marginal distribution of  $Y$  independent of the features is calibrated. [4]  In fact, any model of the form \\[P_X = \\mathrm{law}(Y \\,|\\, \\phi(X)) \\quad \\text{almost surely},\\] where  $\\phi$  is some measurable function, is calibrated."},{"id":54,"pagetitle":"Introduction","title":"Calibration error","ref":"/CalibrationErrors/stable/introduction/#Calibration-error","content":" Calibration error Calibration errors such as the  expected calibration error  and the  kernel calibration error  measure the calibration, or rather the degree of miscalibration, of probabilistic predictive models. They allow a more fine-tuned analysis of calibration and enable comparisons of calibration of different models. Intuitively, calibration measures quantify the deviation of  $P_X$  and  $\\mathrm{law}(Y \\,|\\, P_X)$ , i.e., the left and right hand side in the calibration definition."},{"id":55,"pagetitle":"Introduction","title":"Estimation","ref":"/CalibrationErrors/stable/introduction/#Estimation","content":" Estimation The calibration error of a model depends on the true conditional distribution of targets which is unknown in practice. Therefore calibration errors have to be estimated from a validation data set. Various estimators of different calibration errors such as the  expected calibration error  and the  kernel calibration error  are implemented in CalibrationErrors."},{"id":56,"pagetitle":"Introduction","title":"CalibrationErrors.CalibrationErrorEstimator","ref":"/CalibrationErrors/stable/introduction/#CalibrationErrors.CalibrationErrorEstimator","content":" CalibrationErrors.CalibrationErrorEstimator  —  Type (estimator::CalibrationErrorEstimator)(predictions, targets) Estimate the calibration error of a model from the set of  predictions  and corresponding  targets  using the  estimator . source 1 It does not matter how the model is obtained. In particular, both Bayesian and frequentist approaches can be used. 2 In classification problems, the law  $\\mathrm{law}(Y \\,|\\, X)$  can be identified with a vector in the probability simplex. Therefore often we just consider this equivalent formulation, both for the predictions  $P_X$  and the law  $\\mathrm{law}(Y \\,|\\, X)$ . 3 The general formulation applies not only to classification but to any prediction task with arbitrary target spaces, including regression. 4 In meteorology, this model is called the  climatology ."},{"id":59,"pagetitle":"Kernel calibration error (KCE)","title":"Kernel calibration error (KCE)","ref":"/CalibrationErrors/stable/kce/#kce","content":" Kernel calibration error (KCE)"},{"id":60,"pagetitle":"Kernel calibration error (KCE)","title":"Definition","ref":"/CalibrationErrors/stable/kce/#Definition","content":" Definition The kernel calibration error (KCE) is another calibration error. It is based on real-valued kernels on the product space  $\\mathcal{P} \\times \\mathcal{Y}$  of predictions and targets. The KCE with respect to a real-valued kernel  $k \\colon (\\mathcal{P} \\times \\mathcal{Y}) \\times (\\mathcal{P} \\times \\mathcal{Y}) \\to \\mathbb{R}$  is defined [WLZ21]  as \\[\\mathrm{KCE}_k := \\sup_{f \\in \\mathcal{B}_k} \\bigg| \\mathbb{E}_{Y,P_X} f(P_X, Y) - \\mathbb{E}_{Z_X,P_X} f(P_X, Z_X)\\bigg|,\\] where  $\\mathcal{B}_{k}$  is the unit ball in the  reproducing kernel Hilbert space (RKHS)  to  $k$  and  $Z_X$  is an artificial random variable on the target space  $\\mathcal{Y}$  whose conditional law is given by \\[Z_X \\,|\\, P_X = \\mu \\sim \\mu.\\] The RKHS to kernel  $k$ , and hence also the unit ball  $\\mathcal{B}_k$ , consists of real-valued functions of the form  $f \\colon \\mathcal{P} \\times \\mathcal{Y} \\to \\mathbb{R}$ . For classification models with  $m$  classes, there exists an equivalent formulation of the KCE based on matrix-valued kernel  $\\tilde{k} \\colon \\mathcal{P} \\times \\mathcal{P} \\to \\mathbb{R}^{m \\times m}$  on the space  $\\mathcal{P}$  of predictions. [WLZ19]  The definition above can be rewritten as \\[\\mathrm{KCE}_{\\tilde{k}} := \\sup_{f \\in \\mathcal{B}_{\\tilde{k}}} \\bigg| \\mathbb{E}_{P_X} \\big(\\mathrm{law}(Y \\,|\\, P_X) - P_X\\big)^\\mathsf{T} f(P_X) \\bigg|,\\] where the matrix-valued kernel  $\\tilde{k}$  is given by \\[\\tilde{k}_{i,j}(p, q) = k((p, i), (q, j)) \\quad (i,j=1,\\ldots,m),\\] and  $\\mathcal{B}_{\\tilde{k}}$  is the unit ball in the RKHS of  $\\tilde{k}$ , consisting of vector-valued functions  $f \\colon \\mathcal{P} \\to \\mathbb{R}^m$ . However, this formulation applies only to classification models whereas the general definition above covers all probabilistic predictive models. For a large class of kernels the KCE is zero if and only if the model is calibrated. [WLZ21]  Moreover, the squared KCE (SKCE) can be formulated in terms of the kernel  $k$  as \\[\\begin{aligned}\n\\mathrm{SKCE}_{k} := \\mathrm{KCE}_k^2 &= \\int k(u, v) \\, \\big(\\mathrm{law}(P_X, Y) - \\mathrm{law}(P_X, Z_X)\\big)(u) \\big(\\mathrm{law}(P_X, Y) - \\mathrm{law}(P_X, Z_X)\\big)(v) \\\\\n&= \\mathbb{E} h_k\\big((P_X, Y), (P_{X'}, Y')\\big),\n\\end{aligned}\\] where  $(X',Y')$  is an independent copy of  $(X,Y)$  and \\[\\begin{aligned}\nh_k\\big((\\mu, y), (\\mu', y')\\big) :={}& k\\big((\\mu, y), (\\mu', y')\\big) - \\mathbb{E}_{Z \\sim \\mu} k\\big((\\mu, Z), (\\mu', y')\\big) \\\\\n&- \\mathbb{E}_{Z' \\sim \\mu'} k\\big((\\mu, y), (\\mu', Z')\\big) + \\mathbb{E}_{Z \\sim \\mu, Z' \\sim \\mu'} k\\big((\\mu, Z), (\\mu', Z')\\big).\n\\end{aligned}\\] The KCE is actually a special case of calibration errors that are formulated as integral probability metrics of the form \\[\\sup_{f \\in \\mathcal{F}} \\big| \\mathbb{E}_{Y,P_X} f(P_X, Y) - \\mathbb{E}_{Z_X,P_X} f(P_X, Z_X)\\big|,\\] where  $\\mathcal{F}$  is a space of real-valued functions of the form  $f \\colon \\mathcal{P} \\times \\mathcal{Y} \\to \\mathbb{R}$ . [WLZ21]  For classification models, the  ECE  with respect to common distances such as the total variation distance or the squared Euclidean distance can be formulated in this way. [WLZ19] The maximum mean calibration error (MMCE) [KSJ]  can be viewed as a special case of the KCE, in which only the most-confident predictions are considered. [WLZ19]"},{"id":61,"pagetitle":"Kernel calibration error (KCE)","title":"Estimator","ref":"/CalibrationErrors/stable/kce/#Estimator","content":" Estimator For the SKCE biased and unbiased estimators exist. In CalibrationErrors.jl  SKCE  lets you construct unbiased and biased estimators with quadratic and sub-quadratic sample complexity."},{"id":62,"pagetitle":"Kernel calibration error (KCE)","title":"CalibrationErrors.SKCE","ref":"/CalibrationErrors/stable/kce/#CalibrationErrors.SKCE","content":" CalibrationErrors.SKCE  —  Type SKCE(k; unbiased::Bool=true, blocksize=identity) Estimator of the squared kernel calibration error (SKCE) with kernel  k . Kernel  k  on the product space of predictions and targets has to be a  Kernel  from the Julia package  KernelFunctions.jl  that can be evaluated for inputs that are tuples of predictions and targets. One can choose an unbiased or a biased variant with  unbiased=true  or  unbiased=false , respectively (see details below). The SKCE is estimated as the average estimate of different blocks of samples. The number of samples per block is set by  blocksize : If  blocksize  is a function  blocksize(n::Int) , then the number of samples per block is set to  blocksize(n)  where  n  is the total number of samples. If  blocksize  is an integer, then the number of samplers per block is set to  blocksize , indepedent of the total number of samples. The default setting  blocksize=identity  implies that a single block with all samples is used. The number of samples per block must be at least 1 if  unbiased=false  and 2 if  unbiased=true . Additionally, it must be at most the total number of samples. Note that the last block is neglected if it is incomplete (see details below). Details The unbiased estimator is not guaranteed to be non-negative whereas the biased estimator is always non-negative. The sample complexity of the estimator is  $O(mn)$ , where  $m$  is the block size and  $n$  is the total number of samples. In particular, with the default setting  blocksize=identity  the estimator has a quadratic sample complexity. Let  $(P_{X_i}, Y_i)_{i=1,\\ldots,n}$  be a data set of predictions and corresponding targets. The estimator with block size  $m$  is defined as \\[{\\bigg\\lfloor \\frac{n}{m} \\bigg\\rfloor}^{-1} \\sum_{b=1}^{\\lfloor n/m \\rfloor}\n|B_b|^{-1} \\sum_{(i, j) \\in B_b} h_k\\big((P_{X_i}, Y_i), (P_{X_j}, Y_j)\\big),\\] where \\[\\begin{aligned}\nh_k\\big((μ, y), (μ', y')\\big) ={}&   k\\big((μ, y), (μ', y')\\big)\n                                   - 𝔼_{Z ∼ μ} k\\big((μ, Z), (μ', y')\\big) \\\\\n                                 & - 𝔼_{Z' ∼ μ'} k\\big((μ, y), (μ', Z')\\big)\n                                   + 𝔼_{Z ∼ μ, Z' ∼ μ'} k\\big((μ, Z), (μ', Z')\\big)\n\\end{aligned}\\] and blocks  $B_b$  ( $b = 1, \\ldots, \\lfloor n/m \\rfloor$ ) are defined as \\[B_b = \\begin{cases}\n\\{(i, j): (b - 1) m < i < j \\leq bm \\} & \\text{(unbiased)}, \\\\\n\\{(i, j): (b - 1) m < i, j \\leq bm \\} & \\text{(biased)}.\n\\end{cases}\\] References Widmann, D., Lindsten, F., & Zachariah, D. (2019).  Calibration tests in multi-class classification: A unifying framework . In: Advances in Neural Information Processing Systems (NeurIPS 2019) (pp. 12257–12267). Widmann, D., Lindsten, F., & Zachariah, D. (2021).  Calibration tests beyond classification . source KSJ Kumar, A., Sarawagi, S., & Jain, U. (2018).  Trainable calibration measures for neural networks from kernel mean embeddings . In  Proceedings of the 35th International Conference on Machine Learning  (pp. 2805-2814). WLZ19 Widmann, D., Lindsten, F., & Zachariah, D. (2019).  Calibration tests in multi-class classification: A unifying framework . In  Advances in Neural Information Processing Systems 32 (NeurIPS 2019)  (pp. 12257–12267). WLZ21 Widmann, D., Lindsten, F., & Zachariah, D. (2021).  Calibration tests beyond classification . To be presented at  ICLR 2021 ."},{"id":65,"pagetitle":"Other calibration errors","title":"Other calibration errors","ref":"/CalibrationErrors/stable/others/#Other-calibration-errors","content":" Other calibration errors"},{"id":66,"pagetitle":"Other calibration errors","title":"Unnormalized calibration mean embedding (UCME)","ref":"/CalibrationErrors/stable/others/#Unnormalized-calibration-mean-embedding-(UCME)","content":" Unnormalized calibration mean embedding (UCME) Instead of the formulation of the calibration error as an integral probability metric one can consider the unnormalized calibration mean embedding (UCME). Let  $\\mathcal{P} \\times \\mathcal{Y}$  be the product space of predictions and targets. The UCME for a real-valued kernel  $k \\colon (\\mathcal{P} \\times \\mathcal{Y}) \\times (\\mathcal{P} \\times \\mathcal{Y}) \\to \\mathbb{R}$  and  $m$  test locations is defined [WLZ]  as \\[\\mathrm{UCME}_{k,m}^2 := m^{-1} \\sum_{i=1}^m \\Big(\\mathbb{E}_{Y,P_X} k\\big(T_i, (P_X, Y)\\big) - \\mathbb{E}_{Z_X,P_X} k\\big(T_i, (P_X, Z_X)\\big)\\Big)^2,\\] where test locations  $T_1, \\ldots, T_m$  are i.i.d. random variables whose law is absolutely continuous with respect to the Lebesgue measure on  $\\mathcal{P} \\times \\mathcal{Y}$ . The plug-in estimator of  $\\mathrm{UCME}_{k,m}^2$  is available as  UCME ."},{"id":67,"pagetitle":"Other calibration errors","title":"CalibrationErrors.UCME","ref":"/CalibrationErrors/stable/others/#CalibrationErrors.UCME","content":" CalibrationErrors.UCME  —  Type UCME(k, testpredictions, testtargets) Estimator of the unnormalized calibration mean embedding (UCME) with kernel  k  and sets of  testpredictions  and  testtargets . Kernel  k  on the product space of predictions and targets has to be a  Kernel  from the Julia package  KernelFunctions.jl  that can be evaluated for inputs that are tuples of predictions and targets. The number of test predictions and test targets must be the same and at least one. Details The estimator is biased and guaranteed to be non-negative. Its sample complexity is  $O(mn)$ , where  $m$  is the number of test locations and  $n$  is the total number of samples. Let  $(T_i)_{i=1,\\ldots,m}$  be the set of test locations, i.e., test predictions and corresponding targets, and let  $(P_{X_j}, Y_j)_{j=1,\\ldots,n}$  be a data set of predictions and corresponding targets. The plug-in estimator of  $\\mathrm{UCME}_{k,m}^2$  is defined as \\[m^{-1} \\sum_{i=1}^{m} {\\bigg(n^{-1} \\sum_{j=1}^n k\\big(T_i, (P_{X_j}, Y_j)\\big)\n- \\mathbb{E}_{Z \\sim P_{X_j}} k\\big(T_i, (P_{X_j}, Z)\\big)\\bigg)}^2.\\] References Widmann, D., Lindsten, F., & Zachariah, D. (2021).  Calibration tests beyond classification . To be presented at  ICLR 2021 . source WLZ Widmann, D., Lindsten, F., & Zachariah, D. (2021).  Calibration tests beyond classification . To be presented at  ICLR 2021 ."},{"id":72,"pagetitle":"Home","title":"ReliabilityDiagrams","ref":"/ReliabilityDiagrams/stable/#ReliabilityDiagrams","content":" ReliabilityDiagrams Visualization of model calibration"},{"id":75,"pagetitle":"API","title":"API","ref":"/ReliabilityDiagrams/stable/api/#API","content":" API"},{"id":76,"pagetitle":"API","title":"Diagrams","ref":"/ReliabilityDiagrams/stable/api/#Diagrams","content":" Diagrams"},{"id":77,"pagetitle":"API","title":"Makie","ref":"/ReliabilityDiagrams/stable/api/#Makie","content":" Makie"},{"id":78,"pagetitle":"API","title":"ReliabilityDiagrams.reliability","ref":"/ReliabilityDiagrams/stable/api/#ReliabilityDiagrams.reliability","content":" ReliabilityDiagrams.reliability  —  Function reliability(probabilities, frequencies; deviation=true, kwargs...)\nreliability(probabilities, frequencies, low_high; deviation=true, kwargs...)\nreliability(probabilities, frequencies, low, high; deviation=true, kwargs...) Plot a reliability diagram of the observed  frequencies  versus the predicted  probabilities , optionally with consistency bars ranging from  low  to  high  for each probability. If  deviation  is  true  (default), the difference  frequencies - probabilities  is plotted versus  probabilities . This can be helpful for inspecting how closely the observed frequencies match the predicted probabilities. Attributes Available attributes and their defaults for  Combined{ReliabilityDiagrams.reliability}  are:    barscolor         :black\n  barscolormap      :viridis\n  barscolorrange    MakieCore.Automatic()\n  barslinewidth     1.5\n  barsvisible       true\n  barswhiskerwidth  0\n  color             :black\n  colormap          :viridis\n  colorrange        MakieCore.Automatic()\n  cycle             [:color]\n  deviation         true\n  inspectable       true\n  linestyle         \"nothing\"\n  linewidth         1.5\n  marker            :circle\n  markercolor       :black\n  markercolormap    :viridis\n  markercolorrange  MakieCore.Automatic()\n  markersize        12\n  markervisible     true\n  strokecolor       :black\n  strokewidth       0\n  visible           true General Axis Keywords xlabel : Label of x axis (default:  confidence ) ylabel : Label of y axis (default: if  deviation == true , then  empirical deviation , otherwise  empirical frequency ) Examples julia> probabilities = sort(rand(10));\n\njulia> frequencies = rand(10);\n\njulia> reliability(probabilities, frequencies);\n\njulia> # plot frequencies instead of deviations\n       reliability(probabilities, frequencies; deviation=false);\n\njulia> # use different colors for markers\n       reliability(probabilities, frequencies; markercolor=:red);\n\njulia> # random consistency bars\n       low = max.(0, probabilities .- rand.());\n\njulia> high = min.(1, probabilities .+ rand.());\n\njulia> # plot pre-computed consistency bars\n       reliability(probabilities, frequencies, low, high);\n\njulia> # alternatively consistency bars can be specified as vector of tuples\n       reliability(probabilities, frequencies, map(tuple, low, high)); References Bröcker, J. and Smith, L.A. (2007).  Increasing the reliability of reliability diagrams . Weather and forecasting, 22(3), pp. 651-661. source reliability(\n    probabilities::AbstractVector{<:Real},\n    outcomes::AbstractVector{Bool};\n    binning=EqualMass(),\n    consistencybars=ConsistencyBars(),\n    kwargs...,\n) Plot a reliability diagram of the observed frequencies versus the mean probabilities in different clusters together with a set of consistency bars. Optionally, the binning algorithm that is used to form the clusters and the consistency bars can be configured with the  binning  and  consistencybars  keyword arguments. If  consistencybars === nothing , then no consistency bars are computed. Examples julia> probabilities = rand(10);\n\njulia> outcomes = rand(Bool, 10);\n\njulia> # default binning and consistency bars\n       reliability(probabilities, outcomes);\n\njulia> # custom options: without consistency bars\n       reliability(probabilities, outcomes; consistencybars=nothing); See also:  EqualMass ,  EqualSize ,  ConsistencyBars source"},{"id":79,"pagetitle":"API","title":"Example","ref":"/ReliabilityDiagrams/stable/api/#Example","content":" Example using ReliabilityDiagrams\nusing CairoMakie\n\nprobabilities = rand(100)\noutcomes = rand(100) .< probabilities\nreliability(probabilities, outcomes) lines([0, 1], [0, 1]; color=:black)\nreliability!(probabilities, outcomes; deviation=false) reliability(probabilities, outcomes; consistencybars=nothing) reliability(probabilities, outcomes; binning=EqualSize())"},{"id":80,"pagetitle":"API","title":"Plots","ref":"/ReliabilityDiagrams/stable/api/#Plots","content":" Plots"},{"id":81,"pagetitle":"API","title":"ReliabilityDiagrams.reliabilityplot","ref":"/ReliabilityDiagrams/stable/api/#ReliabilityDiagrams.reliabilityplot","content":" ReliabilityDiagrams.reliabilityplot  —  Function reliabilityplot(probabilities, frequencies; deviation=true, kwargs...)\nreliabilityplot(probabilities, frequencies, low_high; deviation=true, kwargs...)\nreliabilityplot(probabilities, frequencies, low, high; deviation=true, kwargs...) Plot a reliability diagram of the observed  frequencies  versus the predicted  probabilities , optionally with consistency bars ranging from  low  to  high  for each probability. If  deviation  is  true  (default), the difference  frequencies - probabilities  is plotted versus  probabilities . This can be helpful for inspecting how closely the observed frequencies match the predicted probabilities. Examples julia> probabilities = sort(rand(10));\n\njulia> frequencies = rand(10);\n\njulia> reliabilityplot(probabilities, frequencies);\n\njulia> # plot frequencies instead of deviations\n       reliabilityplot(probabilities, frequencies; deviation=false);\n\njulia> # use different colors for markers\n       reliabilityplot(probabilities, frequencies; markercolor=:red);\n\njulia> # random consistency bars\n       low = max.(0, probabilities .- rand.());\n\njulia> high = min.(1, probabilities .+ rand.());\n\njulia> # plot pre-computed consistency bars\n       reliabilityplot(probabilities, frequencies, low, high);\n\njulia> # alternatively consistency bars can be specified as vector of tuples\n       reliabilityplot(probabilities, frequencies, map(tuple, low, high)); References Bröcker, J. and Smith, L.A. (2007).  Increasing the reliability of reliability diagrams . Weather and forecasting, 22(3), pp. 651-661. source reliabilityplot(\n    probabilities::AbstractVector{<:Real},\n    outcomes::AbstractVector{Bool};\n    binning=EqualMass(),\n    consistencybars=ConsistencyBars(),\n    kwargs...,\n) Plot a reliability diagram of the observed frequencies versus the mean probabilities in different clusters together with a set of consistency bars. Optionally, the binning algorithm that is used to form the clusters and the consistency bars can be configured with the  binning  and  consistencybars  keyword arguments. If  consistencybars === nothing , then no consistency bars are computed. Examples julia> probabilities = rand(10);\n\njulia> outcomes = rand(Bool, 10);\n\njulia> # default binning and consistency bars\n       reliabilityplot(probabilities, outcomes);\n\njulia> # custom options: without consistency bars\n       reliabilityplot(probabilities, outcomes; consistencybars=nothing); See also:  EqualMass ,  EqualSize ,  ConsistencyBars source"},{"id":82,"pagetitle":"API","title":"Example","ref":"/ReliabilityDiagrams/stable/api/#Example-2","content":" Example using ReliabilityDiagrams\nusing Plots\n\nprobabilities = rand(100)\noutcomes = rand(100) .< probabilities\nreliabilityplot(probabilities, outcomes) plot([0, 1], [0, 1])\nreliabilityplot!(probabilities, outcomes; deviation=false) reliabilityplot(probabilities, outcomes; consistencybars=nothing) reliabilityplot(probabilities, outcomes; binning=EqualSize())"},{"id":83,"pagetitle":"API","title":"Binning algorithms","ref":"/ReliabilityDiagrams/stable/api/#Binning-algorithms","content":" Binning algorithms"},{"id":84,"pagetitle":"API","title":"ReliabilityDiagrams.EqualMass","ref":"/ReliabilityDiagrams/stable/api/#ReliabilityDiagrams.EqualMass","content":" ReliabilityDiagrams.EqualMass  —  Type EqualMass(; n::Int=10) Create binning algorithm with  n  bins of (approximately) equal mass. source"},{"id":85,"pagetitle":"API","title":"ReliabilityDiagrams.EqualSize","ref":"/ReliabilityDiagrams/stable/api/#ReliabilityDiagrams.EqualSize","content":" ReliabilityDiagrams.EqualSize  —  Type EqualSize(; n::Int=10) Create binning algorithm with  n  bins of the same size. source"},{"id":86,"pagetitle":"API","title":"Consistency bars","ref":"/ReliabilityDiagrams/stable/api/#Consistency-bars","content":" Consistency bars"},{"id":87,"pagetitle":"API","title":"ReliabilityDiagrams.ConsistencyBars","ref":"/ReliabilityDiagrams/stable/api/#ReliabilityDiagrams.ConsistencyBars","content":" ReliabilityDiagrams.ConsistencyBars  —  Type ConsistencyBars(; samples::Int=1_000, coverage::Real=0.95) Create consistency bars that cover a proportion of  coverage  samples obtained by consistency resampling with  samples  resampling steps. References Bröcker, J. and Smith, L.A. (2007).  Increasing the reliability of reliability diagrams . Weather and forecasting, 22(3), pp. 651-661. source"}]